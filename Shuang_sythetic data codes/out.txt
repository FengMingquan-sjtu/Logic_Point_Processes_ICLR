start optimize:
Head = D, base = -0.2000
Rule0: A --> D , A BEFORE D, weight=1.0000
Finish screening one variable, the log likelihood is -10.141790383305683
Params  [tensor([0.0841], dtype=torch.float64, requires_grad=True), tensor([0.1639], dtype=torch.float64, requires_grad=True)]
Current rule is: A --> D , A BEFORE D
feature sum is tensor(-0.0339, dtype=torch.float64)
log-likelihood is  -10.141790383305683
weight = 0.16393404214870694
base = 0.08407546123983729
-------------
Best rule is: A --> D , A BEFORE D
Best log-likelihood = tensor(-0.0339, dtype=torch.float64)
start optimize using cp:
Update Log-likelihood (cvxpy) =  -10.131472979936344
Initialize with this rule:
Head = D, base(torch) = 0.0967, base(cp) = 0.0967,
Rule0: A --> D , A BEFORE D, weight(torch)=0.1613, weight(cp)=0.1613.
start generate simple rule
start calculate intensity log and integral.
start searching.
Current rule is: A --> D , A EQUAL D
feature sum is tensor(-1.8297, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.0186], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  A --> D , A AFTER D
-------------
Current rule is: B --> D , B BEFORE D
feature sum is tensor(6.4803, dtype=torch.float64)
log-likelihood-grad is  tensor([0.2565], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: B --> D , B EQUAL D
feature sum is tensor(-3.8713, dtype=torch.float64)
log-likelihood-grad is  tensor([0.0086], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  B --> D , B AFTER D
-------------
Current rule is: C --> D , C BEFORE D
feature sum is tensor(2.6666, dtype=torch.float64)
log-likelihood-grad is  tensor([0.2022], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: C --> D , C EQUAL D
feature sum is tensor(0.0681, dtype=torch.float64)
log-likelihood-grad is  tensor([0.0245], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  C --> D , C AFTER D
-------------
Current rule is: Not A --> D , Not A BEFORE D
feature sum is tensor(0.9051, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.0078], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: Not A --> D , Not A EQUAL D
feature sum is tensor(-0.9264, dtype=torch.float64)
log-likelihood-grad is  tensor([0.0115], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  Not A --> D , Not A AFTER D
-------------
Current rule is: Not B --> D , Not B BEFORE D
feature sum is tensor(1.5184, dtype=torch.float64)
log-likelihood-grad is  tensor([0.1171], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: Not B --> D , Not B EQUAL D
feature sum is tensor(-1.9077, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.0023], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  Not B --> D , Not B AFTER D
-------------
Current rule is: Not C --> D , Not C BEFORE D
feature sum is tensor(2.4734, dtype=torch.float64)
log-likelihood-grad is  tensor([0.0837], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: Not C --> D , Not C EQUAL D
feature sum is tensor(4.8432, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.0076], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  Not C --> D , Not C AFTER D
-------------
Current rule is: A --> Not D , A BEFORE Not D
feature sum is tensor(-2.9125, dtype=torch.float64)
log-likelihood-grad is  tensor([9.4610e-09], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: A --> Not D , A EQUAL Not D
feature sum is tensor(0.9868, dtype=torch.float64)
log-likelihood-grad is  tensor([0.0186], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  A --> Not D , A AFTER Not D
-------------
Current rule is: B --> Not D , B BEFORE Not D
feature sum is tensor(-2.6786, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.2565], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: B --> Not D , B EQUAL Not D
feature sum is tensor(-0.9140, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.0086], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  B --> Not D , B AFTER Not D
-------------
Current rule is: C --> Not D , C BEFORE Not D
feature sum is tensor(-2.9748, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.2022], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: C --> Not D , C EQUAL Not D
feature sum is tensor(-1.9622, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.0245], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  C --> Not D , C AFTER Not D
-------------
Current rule is: Not A --> Not D , Not A BEFORE Not D
feature sum is tensor(3.3774, dtype=torch.float64)
log-likelihood-grad is  tensor([0.0078], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: Not A --> Not D , Not A EQUAL Not D
feature sum is tensor(-1.9570, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.0115], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  Not A --> Not D , Not A AFTER Not D
-------------
Current rule is: Not B --> Not D , Not B BEFORE Not D
feature sum is tensor(-1.9731, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.1171], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: Not B --> Not D , Not B EQUAL Not D
feature sum is tensor(5.7114, dtype=torch.float64)
log-likelihood-grad is  tensor([0.0023], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  Not B --> Not D , Not B AFTER Not D
-------------
Current rule is: Not C --> Not D , Not C BEFORE Not D
feature sum is tensor(-1.6413, dtype=torch.float64)
log-likelihood-grad is  tensor([-0.0837], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
Current rule is: Not C --> Not D , Not C EQUAL Not D
feature sum is tensor(-1.8730, dtype=torch.float64)
log-likelihood-grad is  tensor([0.0076], dtype=torch.float64, grad_fn=<DivBackward0>)
-------------
This rule is filtered, feature_sum=0.0,  Not C --> Not D , Not C AFTER Not D
-------------
Best rule is: B --> D , B BEFORE D
Best log-likelihood-grad = tensor([0.2565], dtype=torch.float64, grad_fn=<DivBackward0>)
start optimize using cp:
