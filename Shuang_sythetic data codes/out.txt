Current rule is:
Head = C, base = -0.2000
Rule0: A --> C , A BEFORE C, weight=1.0000
feature sum is tensor(11.0779, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: A --> C , A EQUAL C, weight=1.0000
feature sum is tensor(1.8977, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: A --> C , A AFTER C, weight=1.0000
feature sum is tensor(0., dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: B --> C , B BEFORE C, weight=1.0000
feature sum is tensor(25.4394, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: B --> C , B EQUAL C, weight=1.0000
feature sum is tensor(5.7632, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: B --> C , B AFTER C, weight=1.0000
feature sum is tensor(0., dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not A --> C , Not A BEFORE C, weight=1.0000
feature sum is tensor(-0.0652, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not A --> C , Not A EQUAL C, weight=1.0000
feature sum is tensor(2.9189, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not A --> C , Not A AFTER C, weight=1.0000
feature sum is tensor(0., dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not B --> C , Not B BEFORE C, weight=1.0000
feature sum is tensor(-1.4513, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not B --> C , Not B EQUAL C, weight=1.0000
feature sum is tensor(3.9520, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not B --> C , Not B AFTER C, weight=1.0000
feature sum is tensor(0., dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: A --> Not C , A BEFORE Not C, weight=1.0000
feature sum is tensor(-11.1067, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: A --> Not C , A EQUAL Not C, weight=1.0000
feature sum is tensor(-0.8728, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: A --> Not C , A AFTER Not C, weight=1.0000
feature sum is tensor(0., dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: B --> Not C , B BEFORE Not C, weight=1.0000
feature sum is tensor(-26.9944, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: B --> Not C , B EQUAL Not C, weight=1.0000
feature sum is tensor(4.9876, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: B --> Not C , B AFTER Not C, weight=1.0000
feature sum is tensor(0., dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not A --> Not C , Not A BEFORE Not C, weight=1.0000
feature sum is tensor(-3.9530, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not A --> Not C , Not A EQUAL Not C, weight=1.0000
feature sum is tensor(-3.7925, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not A --> Not C , Not A AFTER Not C, weight=1.0000
feature sum is tensor(0., dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not B --> Not C , Not B BEFORE Not C, weight=1.0000
feature sum is tensor(-4.2082, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not B --> Not C , Not B EQUAL Not C, weight=1.0000
feature sum is tensor(0.9113, dtype=torch.float64)
-------------
Current rule is:
Head = C, base = -0.2000
Rule0: Not B --> Not C , Not B AFTER Not C, weight=1.0000
feature sum is tensor(0., dtype=torch.float64)
-------------
start optimize:
Head = C, base = -0.2000
Rule0: B --> C , B BEFORE C, weight=1.0000
batch_gradient =  (tensor([-40.3514], dtype=torch.float64), tensor([29.2399], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 3.114485869187018
batch_gradient =  (tensor([-14.5391], dtype=torch.float64), tensor([2.4231], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 1.9802527017672344
batch_gradient =  (tensor([-18.0402], dtype=torch.float64), tensor([11.3806], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 1.7642830799139582
batch_gradient =  (tensor([3.9663], dtype=torch.float64), tensor([-5.7295], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 1.2251866963491893
batch_gradient =  (tensor([10.0566], dtype=torch.float64), tensor([16.4861], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.9972300001374623
batch_gradient =  (tensor([43.1536], dtype=torch.float64), tensor([6.3577], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.6477768984795443
batch_gradient =  (tensor([19.2455], dtype=torch.float64), tensor([4.6575], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.5795483421681417
batch_gradient =  (tensor([8.2967], dtype=torch.float64), tensor([-13.7453], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.40947588375666294
batch_gradient =  (tensor([11.8178], dtype=torch.float64), tensor([-4.1849], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.36453064838864613
batch_gradient =  (tensor([13.4604], dtype=torch.float64), tensor([-13.0376], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.3137206325500036
batch_gradient =  (tensor([-18.5136], dtype=torch.float64), tensor([-18.9431], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.13521625405302967
batch_gradient =  (tensor([-22.1942], dtype=torch.float64), tensor([-7.1129], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.04479479440207012
Finish screening one variable, the log likelihood is -9.719874447947246
Params  [tensor([0.0501], dtype=torch.float64, requires_grad=True), tensor([0.4913], dtype=torch.float64, requires_grad=True)]
Initialize with this rule:
Head = C, base = 0.0501
Rule0: B --> C , B BEFORE C, weight=0.4913
start generate simple rule
start calculate intensity log and integral.
start searching.
This rule is filtered, feature_sum=0.0,  A --> C , A AFTER C
This rule is filtered, feature_sum=0.0,  B --> C , B AFTER C
This rule is filtered, feature_sum=0.0,  Not A --> C , Not A AFTER C
This rule is filtered, feature_sum=0.0,  Not B --> C , Not B AFTER C
This rule is filtered, feature_sum=0.0,  A --> Not C , A AFTER Not C
This rule is filtered, feature_sum=0.0,  B --> Not C , B AFTER Not C
This rule is filtered, feature_sum=0.0,  Not A --> Not C , Not A AFTER Not C
This rule is filtered, feature_sum=0.0,  Not B --> Not C , Not B AFTER Not C
start optimize:
Head = C, base = 0.0501
Rule0: B --> C , B BEFORE C, weight=0.4913
Rule1: Not A --> C , Not A EQUAL C, weight=0.0100
batch_gradient =  (tensor([-5.6619], dtype=torch.float64), tensor([-11.9547], dtype=torch.float64), tensor([-4.1345], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.8661769438607413
batch_gradient =  (tensor([20.8028], dtype=torch.float64), tensor([2.1660], dtype=torch.float64), tensor([-0.3272], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.5804194088366761
batch_gradient =  (tensor([7.9330], dtype=torch.float64), tensor([-6.0702], dtype=torch.float64), tensor([-1.1747], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.5950019722223848
batch_gradient =  (tensor([14.1888], dtype=torch.float64), tensor([0.7596], dtype=torch.float64), tensor([0.2540], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.6338187295916775
batch_gradient =  (tensor([-23.1033], dtype=torch.float64), tensor([1.8910], dtype=torch.float64), tensor([2.3823], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.24493312922216068
batch_gradient =  (tensor([-11.1987], dtype=torch.float64), tensor([1.4978], dtype=torch.float64), tensor([1.0630], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.12743030911935255
batch_gradient =  (tensor([-10.2982], dtype=torch.float64), tensor([8.1927], dtype=torch.float64), tensor([-0.5910], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.0760791315877849
Finish screening one variable, the log likelihood is -9.513040588130993
Params  [tensor([-0.0256], dtype=torch.float64, requires_grad=True), tensor([0.8087], dtype=torch.float64, requires_grad=True), tensor([0.3313], dtype=torch.float64, requires_grad=True)]
Add a simple rule. Current rule set is:
Head = C, base = -0.0256
Rule0: B --> C , B BEFORE C, weight=0.8087
Rule1: Not A --> C , Not A EQUAL C, weight=0.3313
start generate simple rule
start calculate intensity log and integral.
start searching.
This rule is filtered, feature_sum=0.0,  A --> C , A AFTER C
This rule is filtered, feature_sum=0.0,  B --> C , B AFTER C
This rule is filtered, feature_sum=0.0,  Not A --> C , Not A AFTER C
This rule is filtered, feature_sum=0.0,  Not B --> C , Not B AFTER C
This rule is filtered, feature_sum=0.0,  A --> Not C , A AFTER Not C
This rule is filtered, feature_sum=0.0,  B --> Not C , B AFTER Not C
This rule is filtered, feature_sum=0.0,  Not A --> Not C , Not A AFTER Not C
This rule is filtered, feature_sum=0.0,  Not B --> Not C , Not B AFTER Not C
start optimize:
Head = C, base = -0.0256
Rule0: B --> C , B BEFORE C, weight=0.8087
Rule1: Not A --> C , Not A EQUAL C, weight=0.3313
Rule2: Not A --> Not C , Not A EQUAL Not C, weight=0.0100
batch_gradient =  (tensor([-20.7596], dtype=torch.float64), tensor([2.7741], dtype=torch.float64), tensor([0.2933], dtype=torch.float64), tensor([-0.2933], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 1.309266246312773
batch_gradient =  (tensor([15.6896], dtype=torch.float64), tensor([-6.7230], dtype=torch.float64), tensor([2.4291], dtype=torch.float64), tensor([-2.4291], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.23410628150007406
batch_gradient =  (tensor([25.6688], dtype=torch.float64), tensor([18.8557], dtype=torch.float64), tensor([-4.0100], dtype=torch.float64), tensor([4.0100], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.5310821300425667
batch_gradient =  (tensor([15.5543], dtype=torch.float64), tensor([13.3615], dtype=torch.float64), tensor([-1.0398], dtype=torch.float64), tensor([1.0398], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.7189160665605403
batch_gradient =  (tensor([11.8118], dtype=torch.float64), tensor([2.2694], dtype=torch.float64), tensor([-2.1840], dtype=torch.float64), tensor([2.1840], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.7152236341894779
batch_gradient =  (tensor([-30.1351], dtype=torch.float64), tensor([3.1730], dtype=torch.float64), tensor([0.5694], dtype=torch.float64), tensor([-0.5694], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.4014659076541799
batch_gradient =  (tensor([-25.0401], dtype=torch.float64), tensor([-6.2101], dtype=torch.float64), tensor([0.8351], dtype=torch.float64), tensor([-0.8351], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.25685341488626845
batch_gradient =  (tensor([-22.7907], dtype=torch.float64), tensor([6.6892], dtype=torch.float64), tensor([0.4077], dtype=torch.float64), tensor([-0.4077], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.35661069551024593
batch_gradient =  (tensor([-22.4069], dtype=torch.float64), tensor([-10.5950], dtype=torch.float64), tensor([1.6757], dtype=torch.float64), tensor([-1.6757], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.39925410133191414
batch_gradient =  (tensor([-27.2369], dtype=torch.float64), tensor([-11.9139], dtype=torch.float64), tensor([-0.4926], dtype=torch.float64), tensor([0.4926], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.5032835076125791
batch_gradient =  (tensor([2.2209], dtype=torch.float64), tensor([0.0962], dtype=torch.float64), tensor([0.0883], dtype=torch.float64), tensor([-0.0883], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.4451166552058326
batch_gradient =  (tensor([-13.9301], dtype=torch.float64), tensor([-18.7906], dtype=torch.float64), tensor([-0.9467], dtype=torch.float64), tensor([0.9467], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.4775228611666008
batch_gradient =  (tensor([27.3958], dtype=torch.float64), tensor([-0.9989], dtype=torch.float64), tensor([-1.7320], dtype=torch.float64), tensor([1.7320], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.31115046895416976
batch_gradient =  (tensor([22.6542], dtype=torch.float64), tensor([1.7793], dtype=torch.float64), tensor([0.4068], dtype=torch.float64), tensor([-0.4068], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.18793817471484492
batch_gradient =  (tensor([25.6851], dtype=torch.float64), tensor([-3.1307], dtype=torch.float64), tensor([3.9272], dtype=torch.float64), tensor([-3.9272], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.0758901226204218
Finish screening one variable, the log likelihood is -9.733766849828788
Params  [tensor([0.1823], dtype=torch.float64, requires_grad=True), tensor([0.5343], dtype=torch.float64, requires_grad=True), tensor([0.3500], dtype=torch.float64, requires_grad=True), tensor([-0.0087], dtype=torch.float64, requires_grad=True)]
Add a simple rule. Current rule set is:
Head = C, base = 0.1823
Rule0: B --> C , B BEFORE C, weight=0.5343
Rule1: Not A --> C , Not A EQUAL C, weight=0.3500
Rule2: Not A --> Not C , Not A EQUAL Not C, weight=-0.0087
start generate simple rule
start calculate intensity log and integral.
start searching.
This rule is filtered, feature_sum=0.0,  A --> C , A AFTER C
This rule is filtered, feature_sum=0.0,  B --> C , B AFTER C
This rule is filtered, feature_sum=0.0,  Not A --> C , Not A AFTER C
This rule is filtered, feature_sum=0.0,  Not B --> C , Not B AFTER C
This rule is filtered, feature_sum=0.0,  A --> Not C , A AFTER Not C
This rule is filtered, feature_sum=0.0,  B --> Not C , B AFTER Not C
This rule is filtered, feature_sum=0.0,  Not A --> Not C , Not A AFTER Not C
This rule is filtered, feature_sum=0.0,  Not B --> Not C , Not B AFTER Not C
This rule is filtered, feature_sum=0.0,  A ^ B --> C , A AFTER C ^ B BEFORE C
This rule is filtered, feature_sum=0.0,  Not A ^ B --> C , Not A AFTER C ^ B BEFORE C
This rule is filtered, feature_sum=0.0,  B ^ Not A --> C , B AFTER C ^ Not A EQUAL C
This rule is filtered, feature_sum=0.0,  B ^ Not A --> C , B AFTER Not A ^ Not A EQUAL C
This rule is filtered, feature_sum=0.0,  Not B ^ Not A --> C , Not B EQUAL C ^ Not A EQUAL C
This rule is filtered, feature_sum=0.0,  Not B ^ Not A --> C , Not B AFTER C ^ Not A EQUAL C
This rule is filtered, feature_sum=0.0,  Not B ^ Not A --> C , Not B AFTER Not A ^ Not A EQUAL C
start optimize:
Head = C, base = 0.1823
Rule0: B --> C , B BEFORE C, weight=0.5343
Rule1: Not A --> C , Not A EQUAL C, weight=0.3500
Rule2: Not A --> Not C , Not A EQUAL Not C, weight=-0.0087
Rule3: Not B ^ Not A --> C , Not B EQUAL Not A ^ Not A EQUAL C, weight=0.0100
batch_gradient =  (tensor([33.6497], dtype=torch.float64), tensor([-9.3751], dtype=torch.float64), tensor([0.6464], dtype=torch.float64), tensor([-0.6464], dtype=torch.float64), tensor([-0.1066], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 2.1839622865050434
batch_gradient =  (tensor([0.8922], dtype=torch.float64), tensor([12.7430], dtype=torch.float64), tensor([-0.6298], dtype=torch.float64), tensor([0.6298], dtype=torch.float64), tensor([-0.3729], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 1.0846567684379942
batch_gradient =  (tensor([-12.2766], dtype=torch.float64), tensor([10.5253], dtype=torch.float64), tensor([-0.3108], dtype=torch.float64), tensor([0.3108], dtype=torch.float64), tensor([-0.1244], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.546970641207828
batch_gradient =  (tensor([-22.4675], dtype=torch.float64), tensor([-7.4694], dtype=torch.float64), tensor([-1.5055], dtype=torch.float64), tensor([1.5055], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.10841888809394411
batch_gradient =  (tensor([-8.0146], dtype=torch.float64), tensor([-5.2587], dtype=torch.float64), tensor([-1.6760], dtype=torch.float64), tensor([1.6760], dtype=torch.float64), tensor([0.7171], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.12057406980258888
batch_gradient =  (tensor([-13.1252], dtype=torch.float64), tensor([-0.9674], dtype=torch.float64), tensor([-2.1998], dtype=torch.float64), tensor([2.1998], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.23752462191402576
batch_gradient =  (tensor([-25.1457], dtype=torch.float64), tensor([-11.7324], dtype=torch.float64), tensor([0.4979], dtype=torch.float64), tensor([-0.4979], dtype=torch.float64), tensor([0.8426], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.43270615851307626
batch_gradient =  (tensor([30.1248], dtype=torch.float64), tensor([-10.2243], dtype=torch.float64), tensor([0.1533], dtype=torch.float64), tensor([-0.1533], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.2199462743527361
batch_gradient =  (tensor([-1.0981], dtype=torch.float64), tensor([14.4064], dtype=torch.float64), tensor([4.7828], dtype=torch.float64), tensor([-4.7828], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.13175693832229277
batch_gradient =  (tensor([-11.8556], dtype=torch.float64), tensor([-2.2284], dtype=torch.float64), tensor([3.8744], dtype=torch.float64), tensor([-3.8744], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.1955130138820613
batch_gradient =  (tensor([24.1686], dtype=torch.float64), tensor([3.1427], dtype=torch.float64), tensor([4.7035], dtype=torch.float64), tensor([-4.7035], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.08191810820524187
Finish screening one variable, the log likelihood is -9.697267864256618
Params  [tensor([0.1377], dtype=torch.float64, requires_grad=True), tensor([0.6468], dtype=torch.float64, requires_grad=True), tensor([0.5072], dtype=torch.float64, requires_grad=True), tensor([-0.1659], dtype=torch.float64, requires_grad=True), tensor([0.1396], dtype=torch.float64, requires_grad=True)]
Extend an existing rule. Current rule set is:
Head = C, base = 0.1377
Rule0: B --> C , B BEFORE C, weight=0.6468
Rule1: Not A --> C , Not A EQUAL C, weight=0.5072
Rule2: Not A --> Not C , Not A EQUAL Not C, weight=-0.1659
Rule3: Not B ^ Not A --> C , Not B EQUAL Not A ^ Not A EQUAL C, weight=0.1396
This rule is filtered, feature_sum=0.0,  B ^ Not A --> Not C , B AFTER Not C ^ Not A EQUAL Not C
This rule is filtered, feature_sum=0.0,  B ^ Not A --> Not C , B AFTER Not A ^ Not A EQUAL Not C
This rule is filtered, feature_sum=0.0,  Not B ^ Not A --> Not C , Not B EQUAL Not C ^ Not A EQUAL Not C
This rule is filtered, feature_sum=0.0,  Not B ^ Not A --> Not C , Not B AFTER Not C ^ Not A EQUAL Not C
This rule is filtered, feature_sum=0.0,  Not B ^ Not A --> Not C , Not B AFTER Not A ^ Not A EQUAL Not C
start optimize:
Head = C, base = 0.1377
Rule0: B --> C , B BEFORE C, weight=0.6468
Rule1: Not A --> C , Not A EQUAL C, weight=0.5072
Rule2: Not A --> Not C , Not A EQUAL Not C, weight=-0.1659
Rule3: Not B ^ Not A --> C , Not B EQUAL Not A ^ Not A EQUAL C, weight=0.1396
Rule4: B ^ Not A --> Not C , B EQUAL Not C ^ Not A EQUAL Not C, weight=0.0100
batch_gradient =  (tensor([18.4971], dtype=torch.float64), tensor([-10.1533], dtype=torch.float64), tensor([0.2786], dtype=torch.float64), tensor([-0.2786], dtype=torch.float64), tensor([-0.1082], dtype=torch.float64), tensor([0.1422], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 1.3190614031418955
batch_gradient =  (tensor([-19.7541], dtype=torch.float64), tensor([-2.8352], dtype=torch.float64), tensor([-1.4192], dtype=torch.float64), tensor([1.4192], dtype=torch.float64), tensor([0.], dtype=torch.float64), tensor([0.1343], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.410998018757338
batch_gradient =  (tensor([-11.0874], dtype=torch.float64), tensor([0.5953], dtype=torch.float64), tensor([1.0680], dtype=torch.float64), tensor([-1.0680], dtype=torch.float64), tensor([0.9371], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.36488183099813576
batch_gradient =  (tensor([-10.6820], dtype=torch.float64), tensor([27.2273], dtype=torch.float64), tensor([8.3415], dtype=torch.float64), tensor([-8.3415], dtype=torch.float64), tensor([1.0604], dtype=torch.float64), tensor([-0.6909], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.4663370602611513
batch_gradient =  (tensor([12.5349], dtype=torch.float64), tensor([12.3231], dtype=torch.float64), tensor([2.4480], dtype=torch.float64), tensor([-2.4480], dtype=torch.float64), tensor([-0.0904], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.410924087854972
batch_gradient =  (tensor([6.5846], dtype=torch.float64), tensor([18.2352], dtype=torch.float64), tensor([-1.7873], dtype=torch.float64), tensor([1.7873], dtype=torch.float64), tensor([-0.9475], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.49257742115926867
batch_gradient =  (tensor([18.3169], dtype=torch.float64), tensor([-0.8619], dtype=torch.float64), tensor([0.1691], dtype=torch.float64), tensor([-0.1691], dtype=torch.float64), tensor([0.], dtype=torch.float64), tensor([0.1902], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.43346876304171506
batch_gradient =  (tensor([13.4166], dtype=torch.float64), tensor([1.3373], dtype=torch.float64), tensor([1.5392], dtype=torch.float64), tensor([-1.5392], dtype=torch.float64), tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.43535143542340676
batch_gradient =  (tensor([-1.1531], dtype=torch.float64), tensor([0.2578], dtype=torch.float64), tensor([1.3610], dtype=torch.float64), tensor([-1.3610], dtype=torch.float64), tensor([0.2812], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.38841284424518063
batch_gradient =  (tensor([-30.7669], dtype=torch.float64), tensor([-0.0425], dtype=torch.float64), tensor([-0.0205], dtype=torch.float64), tensor([0.0205], dtype=torch.float64), tensor([0.], dtype=torch.float64), tensor([0.2090], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.30800899108419283
batch_gradient =  (tensor([-3.8429], dtype=torch.float64), tensor([0.4863], dtype=torch.float64), tensor([0.9820], dtype=torch.float64), tensor([-0.9820], dtype=torch.float64), tensor([0.], dtype=torch.float64), tensor([-0.1628], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.28798226899251406
batch_gradient =  (tensor([-13.3859], dtype=torch.float64), tensor([-5.3206], dtype=torch.float64), tensor([-1.3080], dtype=torch.float64), tensor([1.3080], dtype=torch.float64), tensor([-0.0125], dtype=torch.float64), tensor([0.9582], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.25671973880934124
batch_gradient =  (tensor([17.0100], dtype=torch.float64), tensor([-9.8013], dtype=torch.float64), tensor([-2.8389], dtype=torch.float64), tensor([2.8389], dtype=torch.float64), tensor([0.8958], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.16427734719452555
batch_gradient =  (tensor([-3.4326], dtype=torch.float64), tensor([-13.8491], dtype=torch.float64), tensor([-1.9485], dtype=torch.float64), tensor([1.9485], dtype=torch.float64), tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64))
Screening now, the moving avg batch gradient norm is 0.09664048426389528
Finish screening one variable, the log likelihood is -9.544155446057276
Params  [tensor([0.0961], dtype=torch.float64, requires_grad=True), tensor([0.4982], dtype=torch.float64, requires_grad=True), tensor([0.0713], dtype=torch.float64, requires_grad=True), tensor([0.2700], dtype=torch.float64, requires_grad=True), tensor([-0.0406], dtype=torch.float64, requires_grad=True), tensor([-0.2331], dtype=torch.float64, requires_grad=True)]
Extend an existing rule. Current rule set is:
Head = C, base = 0.0961
Rule0: B --> C , B BEFORE C, weight=0.4982
Rule1: Not A --> C , Not A EQUAL C, weight=0.0713
Rule2: Not A --> Not C , Not A EQUAL Not C, weight=0.2700
Rule3: Not B ^ Not A --> C , Not B EQUAL Not A ^ Not A EQUAL C, weight=-0.0406
Rule4: B ^ Not A --> Not C , B EQUAL Not C ^ Not A EQUAL Not C, weight=-0.2331
Maximum rule number reached.
Maximum rule number reached.
Train finished, Final rule set is:
Head = C, base = 0.0961
Rule0: B --> C , B BEFORE C, weight=0.4982
Rule1: Not A --> C , Not A EQUAL C, weight=0.0713
Rule2: Not A --> Not C , Not A EQUAL Not C, weight=0.2700
Rule3: Not B ^ Not A --> C , Not B EQUAL Not A ^ Not A EQUAL C, weight=-0.0406
Rule4: B ^ Not A --> Not C , B EQUAL Not C ^ Not A EQUAL Not C, weight=-0.2331
Head = C, base = 0.0961
Rule0: B --> C , B BEFORE C, weight=0.4982
Rule1: Not A --> C , Not A EQUAL C, weight=0.0713
Rule2: Not A --> Not C , Not A EQUAL Not C, weight=0.2700
Rule3: Not B ^ Not A --> C , Not B EQUAL Not A ^ Not A EQUAL C, weight=-0.0406
Rule4: B ^ Not A --> Not C , B EQUAL Not C ^ Not A EQUAL Not C, weight=-0.2331
