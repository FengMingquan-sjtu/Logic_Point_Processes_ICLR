1. initialize better, no random
2. stopping criter: grad on last 20 batches
print out to see whether decrease
3. return loglikelihood on last 20 batches

debug funcitons one by one.
test simple rule

4. ignore temporal softmax 


[Updated]Softmax on weight leads to error when len(weight) = 1.  Gradient on weight is very small.
Weight gradient represent relavence.

[Fixed]5. SoftMax wrong gradient formula? Must re-derive corrent formula.
Also, Softmax constraints weight to sum to 1 (Prior), this may be wrong in real dataset.
(This is used for samll dataset, where prior is important.)

[Fixed]6. Use 0/1 to represent data state, but use -1/+1 to represent rule.

[Fixed]7. self.Time_tolerance = 0.3 is TOO LARGE. Equal is always matched.
Also, Equal has least decay, therefore feature is large.

[Fixed]8. The heading t=0,state=0 datum prefer to Not predicate.

9. Not E --> Not D  ===  D --> E.  This is bug when D,E are both target and body.
Logic != Causal.

10.Not greedy?
GT = Rule0: A ^ B --> C , A BEFORE C ^ A EQUAL B
But in first round, the potential rules are not selected, since its feature_sum is negative.
This rule is filtered, feature_sum=-0.2548675558009907,  A --> C , A BEFORE C
This rule is filtered, feature_sum=-1.9511806252016348,  A --> C , A EQUAL C
This rule is filtered, feature_sum=0.0,  A --> C , A AFTER C
This rule is filtered, feature_sum=-0.5697855822403801,  B --> C , B BEFORE C

To fixt this, I tried to midify filtering condition to feature_sum==0, thus nagetive feature_sum is allowed,
but still get wrong answer:
Head = C, base = 0.1218
Rule0: Not B --> C , Not B EQUAL C, weight=0.1066
Rule1: B --> C , B BEFORE C, weight=-0.0557
(Let A^B=D, see whether D-->C?  Traditional method can not discover?)
(Modify search process?)
(Maybe data too small)
(Intensity A, B large/small? if A large, B small, then B --> C
transition counts.)
mutual-information?
(1-optimize, relaxation for 1-iter?)


11. The AFTER relation is never matched, featrue is always zero, even no noise.
    This is because, in get_feature() function:
    mask = (transition_time <= cur_time) * (transition_state == template['body_predicate_sign'][idx])
    this line blocks AFTER.
    b-->a has no AFTER.
    only len(body)>2 involves AFTER.

12. intensity larger on event,  better? But what if negative event?
 2 intensities
 2-state markov chain.

13. decay. if too large, forget too fast. 

14. BEFORE--> long-term mem, Equal--> short-term mem.

[Fixed]15. Why B-->C has negative grad?
Because in log-grad, Only calculate feature, missing formula-effect 
=== Test: Head = C, base = -0.0000
Rule0: A --> C , A BEFORE C, weight=1.0000
Rule1: B --> C , B BEFORE C, weight=1.0000 
===
Existing rule :
Head = C, base = 0.1121
Rule0: A --> C , A BEFORE C, weight=0.8428
-------
Current rule is: B --> C , B BEFORE C
feature sum is tensor(4.1275, dtype=torch.float64)
log-likelihood-grad is  tensor([-317.2500], dtype=torch.float64, grad_fn=<AddBackward0>)
-------------


[Fixed]16:
in initialize, some filtered rules are wrongly added to table.  They take idx of 'rules', but do not take idx of 'performance'. Therefore indexing error.

17. After fixing Bug#15, Rule(A->C ^ B->C) can be learned.
    But generate too many simple rules.
    For Rule(A^B->C), it still geenrates too many simple rules, reaching rule num limit.
    Thus searching is ealy-stopped, no rule get expended.
    Print out mid-result.
    In initialize, Best rule is: A --> C , A BEFORE C, Best log-likelihood is 2.7047.
    In first search, Best rule is: B --> C , B BEFORE C, Best log-likelihood-grad = 183.8322
    But the updated log-likelihood is  -10.087690410035039. It is DECREASING !!!
    In the second search, Best rule is: B --> Not C , B BEFORE Not C, Best log-likelihood-grad = 85.9430
    The updated log-likelihood is -10.072779029779538
    In later searches, the result is similar: log-grad is large and positive, but likelihood does not increase too much.
    
    Try to Fix: log-grad is large --> average it, do not use sum. 
        Then, change searching-stopping criterion from (log-grad<=0) to (log-grad <= abs(log-like)*0.01) (No?) 
        log-like decrease after first step: the optimization of init does not converge. Decrease epsilon form 0.1 to 0.05.
    Try DFS?
    Try SoftMax on weights?
    Try CVXPY 

    [Fixed]: compare torch optimization result with cvxpy. torch result is sometimes far from correct.
            decrease epsilon from 0.05 to 0.01 gives more precise result, and solves Rule(A^B->C).
            Also, directly use weight/base of cvxpy also solves Rule(A^B->C).

18. use cvpy to fit Rule(A ^ B --> C , A BEFORE C ^ A EQUAL B).
    threshold is 0. model finds too much simple rules, such that searching stopped for max-rule.
    the last rule has likelihood-grad=[2.8546e-09].
    thus, set threshold=0.01.

19. Mixture searching? BFS+DFS

20. mimicdata, batch-size-cp=1000, killed by system. Out of Mem?
    cvxpy.error.SolverError: Solver 'ECOS' failed. Try another solver, or solve with verbose=True for more information.
    100  +2.005e+03  +2.005e+03  +6e+00  3e-05  2e-03  1e-04  1e-04  0.5013  9e-01   1  0  0 | 18  3
    Maximum number of iterations reached, stopping.
    RAN OUT OF ITERATIONS (reached feastol=1.8e-03, reltol=3.1e-03, abstol=6.1e+00).
    [FIXED]change solver from 'ECOS' to 'SCS'.


21. Complex rules, reuslt is not accurate, more data may help?
    1000 samples --> 5000 samples?

22. [FIXED]Calculating likelihood of all heads at same time is redundant.

23. [FIXED]Too slow.. Run 24h only get 4 simple rules.

24. [FIXED]Extending selection method wrong?

25. dataset?
    online shopping -->  tell story.
    stockings
    mimic dataset more desease.

26. decrease SGD variance, running time.
    avg weight ?
    focus on SGD.

27. [Closed]5 rules, 2w samples. focus on synthetic.

28. [Closed]add regularizer softmax.
    How to deal with [0.3, 0.3, 0.3]?
    the first rule weight must be [1.0, ]

29. hyberid a lstm (Vote vs Base)(only in experiment).
    additional black-box. 
    report percentage from logic. (larger than 50% is promising)

30. release master. keep simple.

31. base 0->1 1->0 are different.

32. mimic: learned rules  --> filtered by expert --> re-fit to get final result.

33. [Added]feature cache
    both intensity-log-sum and intensity-integral will benefit from featrue cache.

34. [Fixed] extending bug.

35. [Update] Feb.21, 0:30. Add avg weight, multi-processing, L1 penalty, Reduce lr to 0.01, Increase batch size to 36

36. AFTER is always zero-feature.
mask = (transition_time <= cur_time) * (transition_state == template['body_predicate_sign'][idx])
[Temporal fix] remove AFTER in enumeration.
[fixed] remove all After relation in codes

37. [Implemented] add a function: delete negative-weight rules.

38. 20000 samples, 
    N-best=1, 24h, get 14 rules.
    N-best=2, 14h, get 15 rules.
    it runs slower when rule set grows larger.
    maybe start from simple rules.

39. featrue-cache + multiprocessing is very slow! race condition.
    time of log-like-grad:
    200 samples
    8 workers, no cache: 9s,8s,8s.
    8 workers, with cache: 2min,5min,8min.
    1 worker(with pool), with cache: 47s
    1 worker(no pool), with cache 33s, 12s,13s,11s.
    1 worker(no pool), no cache 30s,28s,26s.

    maybe first run an arg (write), then multiprocessing(read-only)?
    [Fixed]feature dict key too long?  fix: split dict key into 2 parts.
    [Fixed] temporally remove feature cache from class attr before  multiprocessing.
            since coping feature-cache into sub-processes is slow.
            in multiprocessing, do not use feature-cache, since the common part has already been covered by intensity log and integral, feature-cache is unnecessary.
            after multiprocessing, recover feature-cache.


40. 1 worker(with pool)feature cache size = 78894.
    segmentation fault.
    8 workers(with pool)feature cache size = 280512
    segmentation fault.
    --> multiprocessing pool can not handle large feature cache.


41. [Implemented] log-grad can be redundant. most of rules are useless, but most of time is spent on those rules.

42. with cache and prune: 20000 samples
    3 rules [optimize log-likelihood] Elapsed: 37.1026 min.
    2 rules(delete 1) [optimize log-likelihood] Elapsed: 6.9391 min.
    4 rules(add 2) [optimize log-likelihood] Elapsed: 33.1500 min.
    6 rules(add 2) [optimize log-likelihood] Elapsed: 50.5172 min.
    4 rules(delete 2) [optimize log-likelihood] Elapsed: 33.9437 min.
    6 rules(add 2) [optimize log-likelihood] Elapsed: 41.5354 min.

    8 workers, without cache [multiprocess log-grad] Elapsed: 15min/14min/15min/12min

43. [Fixed]generator intensity is with SoftMax!
    be careful, generator is anther version of intensity function.

44. [Fixed]grad-norm increase with weight num increase.
    avg #weights.

45. Obervation: 
    longer rule is less accurate, requires larger dataset.
    EQUAL is less accurate than BEFORE.
    batch_size 64 is 2-times faster than batch_size 32.
    lr 0.01 is slightly faster than lr 0.005

46. selection of params:
    self.epsilon = 0.01 --> modify to 0.001
    self.gain_threshold = 0.01 --> modify to 0.1
    self.low_grad_threshold = 1e-5 --> modify to 0.01
    self.weight_threshold = 1e-5 --> modify to 0.01
    seems too heavily prune. 
    esp for long rules, single composition may have low weight or log-grad.
    e.g. B^C^D-->E, composition log-grad is B^C(0.07,0.058,0.049), D^C(0.01), D^B(0.09,0.08) 
    
    [Update]self.gain_threshold = 0.1 --> modify to 0.05
    B^C^D (0.029,0.028)
    [Update]self.gain_threshold = 0.05 --> modify to 0.02
    self.epsilon = 0.001 --> 0.005
    self.num_batch_check_for_update = 20 --> 300
    change stop criteria from AND to OR.
    self.epsilon = 0.005 --> 0.003

    self.integral_resolution =0.3 --> 0.1

47. Auto-stop of SGD
    [Implemented] (20 batches no update best likelihood) OR (grad-norm < epsilon)
    [Fixed] seems AND is better. add iter_num? --> iter_num=30 for 2000samples, iter_num=60 for 1000samples.

48. [implemented] Auto-stop of sub-problem.
    multi-arm-bandit
    UCB
    can save time, from 1min to 20sec.
    when grad is small e.g. 0.01, variance is still large, e.g. 1. then result is not accurate enough.
    in a typical run, opt time is 10min, log-grad time is 2 min.
    fast but inaccurate UCB saves log-grad time, but wastes opt time.
    [disabled at synthetic data] with parallel calculation, UCB is slower than calculate all data.
    UCB needs at least 2 rounds(init+other) of parallel. benefit of UCB is offset by delay of rounds.
    

49. [Implemented]Fit generated data with GT rules, to check data variance.
    seems len-3 rule is under-estimated. 2000 samples, 45 iter converge. 

50. [Implemented]add redundant variables, test wether model can ignore it.

51. Association rule learning(With Temporal) is anther baseline?

52. [Implemented]sort weight descendingly, first extend the largest-weight rule in 2nd phase.

53. [Implemented]When training finishes, heavily prune weight<0.1 and re-fit.
    the noise rules may disturb correct rules weight.
    and also set a large iter_num(e.g. 20) for better acc

54. Again duplicate bug !!!
    modify generate_rule_via_column_generation() but forget to modify its copy in add_one_predicate_to_existing_rule()
    [FIXED] make the duplicate part as a function.

55. [Observation]
    Addition of correct new rule, and increase of its weights, 
    leads to decrease of incorrect and composite rule weights (due to l1 penalty and data coverage),
    which further leads to improvements on other correct rule weights.

56. generate 1000 samples, 5 formulas, 6 preds, cost 10 hours.
    [Update] disable cache in generator to save time.

57. [updated]max_rule_num=20 is not enough. --> modify to 30
    also, if num_formula >= max_num_rule, strictly prune.

58. iterrative update calculation of intensity?

59. in paper, report a single sample analysis?

60. print grad in likelihood-opt, to check which variable is not converged.
    use tensorboard 

61. good and cheap initialization
    [Updated] no need initialization! directly start. since intensity is never zero, no zero-divition-error.

62. [Implemented] parallel generator, optimize_log_likelihood(2x speed), and get_intensity_and_integral_grad (3x speed).
    accelerate.

63. systemetic synthetic experiment

64. inconsistent result of multiprocess opt and opt.
    maybe because, the first worker_num batches are started with same weights, but the following batches are started sequentially.
    mp-opt:
    grad norm=2.5704365477001274. num_batch_no_update =0
    Finish optimize_log_likelihood, the log likelihood is -9.413276336477095
    Params  [tensor([-0.2085], dtype=torch.float64, requires_grad=True), tensor([0.0550], dtype=torch.float64, requires_grad=True)]

    opt:
    grad norm=2.643148751161549. num_batch_no_update =0
    Finish optimize_log_likelihood, the log likelihood is -9.406897454891597
    Params  [tensor([-0.2309], dtype=torch.float64, requires_grad=True), tensor([0.0499], dtype=torch.float64, requires_grad=True)]
    
    likelihood div batch-size? otherwise lr is batch-size*lr
    currently, we do not div batch-size, and use batch-size=64, lr=0.005

    maybe because batch_size should not too small.

    successfully recovered data-2, using 8 workers, each with batch-size-mp=64.
    for 20 ruels, 75 batches, 8-worker-mp use 7min, single-process-cache costs above 10min.

65. set time-window[Implemented], set smaller weights\intensity to get less events, to avoid too much computation.
    TODO: need to re-generate datasets. 

66. add AFTER experiment?
    [updated] remove AFTER experiment

67. larger best_N?  e.g. 8,  or even add all positive rules? (need theoreticall analysis.)

68. [implemented]add seq-avg-len 

69. modify feature calculation to O(n)

70. draw likelihood function image, verify deletting/adding rule criteria.

71. [Implemented]in featrue fucntion, filter predicate if it has time relation with target.
    add filter, reduce generating data-2 from 5h to 50min
    time_window=10
    [Generate data] Elapsed: 55.7610 min.
    [Generate data] Elapsed: 4.3923 hour.

72. generate data-2 costs long time.
    reason?
    time_window=7,8 does not fit.
    time_window=10 works 

73. in enumeration candidate rules, do not enumerate target sign, since this is redundant, by symmetricity.
    the symmetricity is "grad(R->Y) = - grad(R->notY)", it is guaranteed by (wf) term and chain-rule. 
    after calculation of log-grad, for all negative log-grad rules, revert its target sign and log-grad sign.
    Note: this is risky if there are contradictory rules in rule set. But in old verison, such risk was also not considered.
    [update] due to repeated rule bug, discard this fix.

74. fit failed for 1280, 640 samples of data-4
    2560 samples killed by system?? in [multiprocess log-grad] 40 tasks, only get 17 result and quit.
    still use cache??

75. after run setsid, check model-info and running states of program!!

76. use cp instead of torch to solve master.
    note: no l1 panelty. [Fixed: added]
    fit GT rules, 2560 samples cost 10+G memory.
    mosek solver is 3x faster than scs.
    
77. when adding contradictory rules, e.g.
    Best rule is: D ^ Not A --> E , D EQUAL Not A ^ Not A EQUAL E
    Best log-likelihood-grad(all-data) = 0.04151670803228344
    new rule added.
    Best rule is: Not D ^ Not A --> E , Not D EQUAL Not A ^ Not A EQUAL E
    Best log-likelihood-grad(all-data) = 0.039954239767975484
    new rule added.
    mosek returns:
    Problem status: The problem is primal infeasible.

    [Implemented]So we constraint candidate rules not contradictory.

78. run multiprocessing program on ubuntu system, ulimit -n = 1024,  gives:
    RuntimeError: received 0 items of ancdata.
    which means number of file descriptors is larger than limit.
    Also, this error is not outputed to stderr or stdout.
    [Fixed]solution: replace multiprocessing with torch.multiprocessing,
    and adding the following line in main function:
    ``torch.multiprocessing.set_sharing_strategy('file_system')``
    (solution comes from https://forums.fast.ai/t/runtimeerror-received-0-items-of-ancdata/48935/2)

79. most of failure results from long rules.
    [Implemented]use DFS to probe long rule?
    the old search_algorithm keeps extending a short rule, even after it is deleted.
    [Fixed]fix this bug and modfiy search_algorithm to BFS.

80. data-6 2400 samples, run out of shared-mem?
    re-run same codes, not reproduce.

81. [Implemented] strict_weight_threshold should change with dataset weight scale.