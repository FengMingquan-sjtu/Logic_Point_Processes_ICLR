1. initialize better, no random
2. stopping criter: grad on last 20 batches
print out to see whether decrease
3. return loglikelihood on last 20 batches

debug funcitons one by one.
test simple rule

4. ignore temporal softmax 


[Updated]Softmax on weight leads to error when len(weight) = 1.  Gradient on weight is very small.
Weight gradient represent relavence.

[Fixed]5. SoftMax wrong gradient formula? Must re-derive corrent formula.
Also, Softmax constraints weight to sum to 1 (Prior), this may be wrong in real dataset.
(This is used for samll dataset, where prior is important.)

[Fixed]6. Use 0/1 to represent data state, but use -1/+1 to represent rule.

[Fixed]7. self.Time_tolerance = 0.3 is TOO LARGE. Equal is always matched.
Also, Equal has least decay, therefore feature is large.

[Fixed]8. The heading t=0,state=0 datum prefer to Not predicate.

9. Not E --> Not D  ===  D --> E.  This is bug when D,E are both target and body.
Logic != Causal.

10.Not greedy?
GT = Rule0: A ^ B --> C , A BEFORE C ^ A EQUAL B
But in first round, the potential rules are not selected, since its feature_sum is negative.
This rule is filtered, feature_sum=-0.2548675558009907,  A --> C , A BEFORE C
This rule is filtered, feature_sum=-1.9511806252016348,  A --> C , A EQUAL C
This rule is filtered, feature_sum=0.0,  A --> C , A AFTER C
This rule is filtered, feature_sum=-0.5697855822403801,  B --> C , B BEFORE C

To fixt this, I tried to midify filtering condition to feature_sum==0, thus nagetive feature_sum is allowed,
but still get wrong answer:
Head = C, base = 0.1218
Rule0: Not B --> C , Not B EQUAL C, weight=0.1066
Rule1: B --> C , B BEFORE C, weight=-0.0557
(Let A^B=D, see whether D-->C?  Traditional method can not discover?)
(Modify search process?)
(Maybe data too small)
(Intensity A, B large/small? if A large, B small, then B --> C
transition counts.)
mutual-information?
(1-optimize, relaxation for 1-iter?)


11. The AFTER relation is never matched, featrue is always zero, even no noise.
    This is because, in get_feature() function:
    mask = (transition_time <= cur_time) * (transition_state == template['body_predicate_sign'][idx])
    this line blocks AFTER.
    b-->a has no AFTER.
    only len(body)>2 involves AFTER.

12. intensity larger on event,  better? But what if negative event?
 2 intensities
 2-state markov chain.

13. decay. if too large, forget too fast. 

14. BEFORE--> long-term mem, AFTER--> short-term mem.

[Fixed]15. Why B-->C has negative grad?
Because in log-grad, Only calculate feature, missing formula-effect 
=== Test: Head = C, base = -0.0000
Rule0: A --> C , A BEFORE C, weight=1.0000
Rule1: B --> C , B BEFORE C, weight=1.0000 
===
Existing rule :
Head = C, base = 0.1121
Rule0: A --> C , A BEFORE C, weight=0.8428
-------
Current rule is: B --> C , B BEFORE C
feature sum is tensor(4.1275, dtype=torch.float64)
log-likelihood-grad is  tensor([-317.2500], dtype=torch.float64, grad_fn=<AddBackward0>)
-------------


[Fixed]16:
in initialize, some filtered rules are wrongly added to table.  They take idx of 'rules', but do not take idx of 'performance'. Therefore indexing error.

17. After fixing Bug#15, Rule(A->C ^ B->C) can be learned.
    But generate too many simple rules.
    For Rule(A^B->C), it still geenrates too many simple rules, reaching rule num limit.
    Thus searching is ealy-stopped, no rule get expended.
    Print out mid-result.
    In initialize, Best rule is: A --> C , A BEFORE C, Best log-likelihood is 2.7047.
    In first search, Best rule is: B --> C , B BEFORE C, Best log-likelihood-grad = 183.8322
    But the updated log-likelihood is  -10.087690410035039. It is DECREASING !!!
    In the second search, Best rule is: B --> Not C , B BEFORE Not C, Best log-likelihood-grad = 85.9430
    The updated log-likelihood is -10.072779029779538
    In later searches, the result is similar: log-grad is large and positive, but likelihood does not increase too much.
    
    Try to Fix: log-grad is large --> average it, do not use sum. 
        Then, change searching-stopping criterion from (log-grad<=0) to (log-grad <= abs(log-like)*0.01) (No?) 
        log-like decrease after first step: the optimization of init does not converge. Decrease epsilon form 0.1 to 0.05.
    Try DFS?
    Try SoftMax on weights?
    Try CVXPY 

    [Fixed]: compare torch optimization result with cvxpy. torch result is sometimes far from correct.
            decrease epsilon from 0.05 to 0.01 gives more precise result, and solves Rule(A^B->C).
            Also, directly use weight/base of cvxpy also solves Rule(A^B->C).

18. use cvpy to fit Rule(A ^ B --> C , A BEFORE C ^ A EQUAL B).
    threshold is 0. model finds too much simple rules, such that searching stopped for max-rule.
    the last rule has likelihood-grad=[2.8546e-09].
    thus, set threshold=0.01.

19. Mixture searching? BFS+DFS

20. mimicdata, batch-size-cp=1000, killed by system. Out of Mem?
    cvxpy.error.SolverError: Solver 'ECOS' failed. Try another solver, or solve with verbose=True for more information.
    100  +2.005e+03  +2.005e+03  +6e+00  3e-05  2e-03  1e-04  1e-04  0.5013  9e-01   1  0  0 | 18  3
    Maximum number of iterations reached, stopping.
    RAN OUT OF ITERATIONS (reached feastol=1.8e-03, reltol=3.1e-03, abstol=6.1e+00).
    [FIXED]change solver from 'ECOS' to 'SCS'.


21. Complex rules, reuslt is not accurate, more data may help?
    1000 samples --> 5000 samples?

22. [FIXED]Calculating likelihood of all heads at same time is redundant.

23. Too slow.. Run 24h only get 4 simple rules.

24. Extending selection method wrong?

25. dataset?
    online shopping -->  tell story.
    mimic dataset more desease.

26. decrease SGD variance, running time.
    avg weight ?
    focus on SGD.

27. 5 rules, 2w samples. focus on synthetic.

28. add regularizer softmax.
    How to deal with [0.3, 0.3, 0.3]?
    the first rule weight must be [1.0, ]

29. hyberid a lstm (Vote vs Base)(only in experiment).
    additional black-box. 
    report percentage from logic. (larger than 50% is promising)

30. release master. keep simple.

31. base 0->1 1->0 are different.

32. mimic: learned rules  --> filtered by expert --> re-fit to get final result.

33. [Added]feature cache
    both intensity-log-sum and intensity-integral will benefit from featrue cache.

34. [Fixed] extending bug.

35. [Update] Feb.21, 0:30. Add avg weight, multi-processing, L1 penalty, Reduce lr to 0.01, Increase batch size to 36

36. AFTER is always zero-feature.
mask = (transition_time <= cur_time) * (transition_state == template['body_predicate_sign'][idx])
[Temporal fix] remove AFTER in enumeration.

37. [Implemented] add a function: delete negative-weight rules.

38. 20000 samples, N-best=1, 24h, get 14 rules. N-best=2, 14h, get 15 rules.
    it runs slower when rule set grows larger.
    maybe start from simple rules.

39. featrue-cache + multiprocessing is very slow! race condition.
    time of log-like-grad:
    200 samples
    8 workers, no cache: 9s,8s,8s.
    8 workers, with cache: 2min,5min,8min.
    1 worker(with pool), with cache: 47s
    1 worker(no pool), with cache 33s, 12s,13s,11s.
    1 worker(no pool), no cache 30s,28s,26s.

    maybe first run an arg (write), then multiprocessing(read-only)?
    [Fixed]feature dict key too long?  fix: split dict key into 2 parts.
    [Fixed] temporally remove feature cache from class attr before  multiprocessing.
            since coping feature-cache into sub-processes is slow.
            in multiprocessing, do not use feature-cache, since the common part has already been covered by intensity log and integral, feature-cache is unnecessary.
            after multiprocessing, recover feature-cache.


40. 1 worker(with pool)feature cache size = 78894.
    segmentation fault.
    8 workers(with pool)feature cache size = 280512
    segmentation fault.
    --> multiprocessing pool can not handle large feature cache.


41. [Implemented] log-grad can be redundant. most of rules are useless, but most of time is spent on those rules.
    