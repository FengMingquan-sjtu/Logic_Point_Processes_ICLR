1. initialize better, no random
2. stopping criter: grad on last 20 batches
print out to see whether decrease
3. return loglikelihood on last 20 batches

debug funcitons one by one.
test simple rule

4. ignore temporal softmax 


[Updated]Softmax on weight leads to error when len(weight) = 1.  Gradient on weight is very small.
Weight gradient represent relavence.

[Fixed]5. SoftMax wrong gradient formula? Must re-derive corrent formula.
Also, Softmax constraints weight to sum to 1 (Prior), this may be wrong in real dataset.
(This is used for samll dataset, where prior is important.)

[Fixed]6. Use 0/1 to represent data state, but use -1/+1 to represent rule.

[Fixed]7. self.Time_tolerance = 0.3 is TOO LARGE. Equal is always matched.
Also, Equal has least decay, therefore feature is large.

[Fixed]8. The heading t=0,state=0 datum prefer to Not predicate.

9. Not E --> Not D  ===  D --> E.  This is bug when D,E are both target and body.
Logic != Causal.

10.Not greedy?
GT = Rule0: A ^ B --> C , A BEFORE C ^ A EQUAL B
But in first round, the potential rules are not selected, since its feature_sum is negative.
This rule is filtered, feature_sum=-0.2548675558009907,  A --> C , A BEFORE C
This rule is filtered, feature_sum=-1.9511806252016348,  A --> C , A EQUAL C
This rule is filtered, feature_sum=0.0,  A --> C , A AFTER C
This rule is filtered, feature_sum=-0.5697855822403801,  B --> C , B BEFORE C

To fixt this, I tried to midify filtering condition to feature_sum==0, thus nagetive feature_sum is allowed,
but still get wrong answer:
Head = C, base = 0.1218
Rule0: Not B --> C , Not B EQUAL C, weight=0.1066
Rule1: B --> C , B BEFORE C, weight=-0.0557
(Let A^B=D, see whether D-->C?  Traditional method can not discover?)
(Modify search process?)
(Maybe data too small)
(Intensity A, B large/small? if A large, B small, then B --> C
transition counts.)
mutual-information?
(1-optimize, relaxation for 1-iter?)


11. The AFTER relation is never matched, featrue is always zero, even no noise.
    This is because, in get_feature() function:
    mask = (transition_time <= cur_time) * (transition_state == template['body_predicate_sign'][idx])
    this line blocks AFTER.
    b-->a has no AFTER.
    only len(body)>2 involves AFTER.

12. intensity larger on event,  better? But what if negative event?
 2 intensities
 2-state markov chain.

13. decay. if too large, forget too fast. 

14. BEFORE--> long-term mem, Equal--> short-term mem.

[Fixed]15. Why B-->C has negative grad?
Because in log-grad, Only calculate feature, missing formula-effect 
=== Test: Head = C, base = -0.0000
Rule0: A --> C , A BEFORE C, weight=1.0000
Rule1: B --> C , B BEFORE C, weight=1.0000 
===
Existing rule :
Head = C, base = 0.1121
Rule0: A --> C , A BEFORE C, weight=0.8428
-------
Current rule is: B --> C , B BEFORE C
feature sum is tensor(4.1275, dtype=torch.float64)
log-likelihood-grad is  tensor([-317.2500], dtype=torch.float64, grad_fn=<AddBackward0>)
-------------


[Fixed]16:
in initialize, some filtered rules are wrongly added to table.  They take idx of 'rules', but do not take idx of 'performance'. Therefore indexing error.

17. After fixing Bug#15, Rule(A->C ^ B->C) can be learned.
    But generate too many simple rules.
    For Rule(A^B->C), it still geenrates too many simple rules, reaching rule num limit.
    Thus searching is ealy-stopped, no rule get expended.
    Print out mid-result.
    In initialize, Best rule is: A --> C , A BEFORE C, Best log-likelihood is 2.7047.
    In first search, Best rule is: B --> C , B BEFORE C, Best log-likelihood-grad = 183.8322
    But the updated log-likelihood is  -10.087690410035039. It is DECREASING !!!
    In the second search, Best rule is: B --> Not C , B BEFORE Not C, Best log-likelihood-grad = 85.9430
    The updated log-likelihood is -10.072779029779538
    In later searches, the result is similar: log-grad is large and positive, but likelihood does not increase too much.
    
    Try to Fix: log-grad is large --> average it, do not use sum. 
        Then, change searching-stopping criterion from (log-grad<=0) to (log-grad <= abs(log-like)*0.01) (No?) 
        log-like decrease after first step: the optimization of init does not converge. Decrease epsilon form 0.1 to 0.05.
    Try DFS?
    Try SoftMax on weights?
    Try CVXPY 

    [Fixed]: compare torch optimization result with cvxpy. torch result is sometimes far from correct.
            decrease epsilon from 0.05 to 0.01 gives more precise result, and solves Rule(A^B->C).
            Also, directly use weight/base of cvxpy also solves Rule(A^B->C).

18. use cvpy to fit Rule(A ^ B --> C , A BEFORE C ^ A EQUAL B).
    threshold is 0. model finds too much simple rules, such that searching stopped for max-rule.
    the last rule has likelihood-grad=[2.8546e-09].
    thus, set threshold=0.01.

19. Mixture searching? BFS+DFS

20. mimicdata, batch-size-cp=1000, killed by system. Out of Mem?
    cvxpy.error.SolverError: Solver 'ECOS' failed. Try another solver, or solve with verbose=True for more information.
    100  +2.005e+03  +2.005e+03  +6e+00  3e-05  2e-03  1e-04  1e-04  0.5013  9e-01   1  0  0 | 18  3
    Maximum number of iterations reached, stopping.
    RAN OUT OF ITERATIONS (reached feastol=1.8e-03, reltol=3.1e-03, abstol=6.1e+00).
    [FIXED]change solver from 'ECOS' to 'SCS'.


21. Complex rules, reuslt is not accurate, more data may help?
    1000 samples --> 5000 samples?

22. [FIXED]Calculating likelihood of all heads at same time is redundant.

23. [FIXED]Too slow.. Run 24h only get 4 simple rules.

24. [FIXED]Extending selection method wrong?

25. dataset?
    online shopping -->  tell story.
    stockings
    mimic dataset more desease.

26. decrease SGD variance, running time.
    avg weight ?
    focus on SGD.

27. [Closed]5 rules, 2w samples. focus on synthetic.

28. [Closed]add regularizer softmax.
    How to deal with [0.3, 0.3, 0.3]?
    the first rule weight must be [1.0, ]

29. hyberid a lstm (Vote vs Base)(only in experiment).
    additional black-box. 
    report percentage from logic. (larger than 50% is promising)

30. release master. keep simple.

31. base 0->1 1->0 are different.

32. mimic: learned rules  --> filtered by expert --> re-fit to get final result.

33. [Added]feature cache
    both intensity-log-sum and intensity-integral will benefit from featrue cache.

34. [Fixed] extending bug.

35. [Update] Feb.21, 0:30. Add avg weight, multi-processing, L1 penalty, Reduce lr to 0.01, Increase batch size to 36

36. AFTER is always zero-feature.
mask = (transition_time <= cur_time) * (transition_state == template['body_predicate_sign'][idx])
[Temporal fix] remove AFTER in enumeration.
[fixed] remove all After relation in codes

37. [Implemented] add a function: delete negative-weight rules.

38. 20000 samples, 
    N-best=1, 24h, get 14 rules.
    N-best=2, 14h, get 15 rules.
    it runs slower when rule set grows larger.
    maybe start from simple rules.

39. featrue-cache + multiprocessing is very slow! race condition.
    time of log-like-grad:
    200 samples
    8 workers, no cache: 9s,8s,8s.
    8 workers, with cache: 2min,5min,8min.
    1 worker(with pool), with cache: 47s
    1 worker(no pool), with cache 33s, 12s,13s,11s.
    1 worker(no pool), no cache 30s,28s,26s.

    maybe first run an arg (write), then multiprocessing(read-only)?
    [Fixed]feature dict key too long?  fix: split dict key into 2 parts.
    [Fixed] temporally remove feature cache from class attr before  multiprocessing.
            since coping feature-cache into sub-processes is slow.
            in multiprocessing, do not use feature-cache, since the common part has already been covered by intensity log and integral, feature-cache is unnecessary.
            after multiprocessing, recover feature-cache.


40. 1 worker(with pool)feature cache size = 78894.
    segmentation fault.
    8 workers(with pool)feature cache size = 280512
    segmentation fault.
    --> multiprocessing pool can not handle large feature cache.


41. [Implemented] log-grad can be redundant. most of rules are useless, but most of time is spent on those rules.

42. with cache and prune: 20000 samples
    3 rules [optimize log-likelihood] Elapsed: 37.1026 min.
    2 rules(delete 1) [optimize log-likelihood] Elapsed: 6.9391 min.
    4 rules(add 2) [optimize log-likelihood] Elapsed: 33.1500 min.
    6 rules(add 2) [optimize log-likelihood] Elapsed: 50.5172 min.
    4 rules(delete 2) [optimize log-likelihood] Elapsed: 33.9437 min.
    6 rules(add 2) [optimize log-likelihood] Elapsed: 41.5354 min.

    8 workers, without cache [multiprocess log-grad] Elapsed: 15min/14min/15min/12min

43. [Fixed]generator intensity is with SoftMax!
    be careful, generator is anther version of intensity function.

44. [Fixed]grad-norm increase with weight num increase.
    avg #weights.

45. Obervation: 
    longer rule is less accurate, requires larger dataset.
    EQUAL is less accurate than BEFORE.
    batch_size 64 is 2-times faster than batch_size 32.
    lr 0.01 is slightly faster than lr 0.005

46. selection of params:
    self.epsilon = 0.01 --> modify to 0.001
    self.gain_threshold = 0.01 --> modify to 0.1
    self.low_grad_threshold = 1e-5 --> modify to 0.01
    self.weight_threshold = 1e-5 --> modify to 0.01
    seems too heavily prune. 
    esp for long rules, single composition may have low weight or log-grad.
    e.g. B^C^D-->E, composition log-grad is B^C(0.07,0.058,0.049), D^C(0.01), D^B(0.09,0.08) 
    
    [Update]self.gain_threshold = 0.1 --> modify to 0.05
    B^C^D (0.029,0.028)
    [Update]self.gain_threshold = 0.05 --> modify to 0.02
    self.epsilon = 0.001 --> 0.005
    self.num_batch_check_for_update = 20 --> 300
    change stop criteria from AND to OR.
    self.epsilon = 0.005 --> 0.003

    self.integral_resolution =0.3 --> 0.1

47. Auto-stop of SGD
    [Implemented] (20 batches no update best likelihood) OR (grad-norm < epsilon)
    [Fixed] seems AND is better. add iter_num? --> iter_num=30 for 2000samples, iter_num=60 for 1000samples.

48. [implemented] Auto-stop of sub-problem.
    multi-arm-bandit
    UCB
    can save time, from 1min to 20sec.
    when grad is small e.g. 0.01, variance is still large, e.g. 1. then result is not accurate enough.
    in a typical run, opt time is 10min, log-grad time is 2 min.
    fast but inaccurate UCB saves log-grad time, but wastes opt time.
    [disabled at synthetic data] with parallel calculation, UCB is slower than calculate all data.
    UCB needs at least 2 rounds(init+other) of parallel. benefit of UCB is offset by delay of rounds.
    

49. [Implemented]Fit generated data with GT rules, to check data variance.
    seems len-3 rule is under-estimated. 2000 samples, 45 iter converge. 

50. [Implemented]add redundant variables, test wether model can ignore it.

51. Association rule learning(With Temporal) is anther baseline?

52. [Implemented]sort weight descendingly, first extend the largest-weight rule in 2nd phase.

53. [Implemented]When training finishes, heavily prune weight<0.1 and re-fit.
    the noise rules may disturb correct rules weight.
    and also set a large iter_num(e.g. 20) for better acc

54. Again duplicate bug !!!
    modify generate_rule_via_column_generation() but forget to modify its copy in add_one_predicate_to_existing_rule()
    [FIXED] make the duplicate part as a function.

55. [Observation]
    Addition of correct new rule, and increase of its weights, 
    leads to decrease of incorrect and composite rule weights (due to l1 penalty and data coverage),
    which further leads to improvements on other correct rule weights.

56. generate 1000 samples, 5 formulas, 6 preds, cost 10 hours.
    [Update] disable cache in generator to save time.

57. [updated]max_rule_num=20 is not enough. --> modify to 30
    also, if num_formula >= max_num_rule, strictly prune.

58. iterrative update calculation of intensity?

59. in paper, report a single sample analysis?

60. print grad in likelihood-opt, to check which variable is not converged.
    use tensorboard 

61. good and cheap initialization
    [Updated] no need initialization! directly start. since intensity is never zero, no zero-divition-error.

62. [Implemented] parallel generator, optimize_log_likelihood(2x speed), and get_intensity_and_integral_grad (3x speed).
    accelerate.

63. systemetic synthetic experiment

64. inconsistent result of multiprocess opt and opt.
    maybe because, the first worker_num batches are started with same weights, but the following batches are started sequentially.
    mp-opt:
    grad norm=2.5704365477001274. num_batch_no_update =0
    Finish optimize_log_likelihood, the log likelihood is -9.413276336477095
    Params  [tensor([-0.2085], dtype=torch.float64, requires_grad=True), tensor([0.0550], dtype=torch.float64, requires_grad=True)]

    opt:
    grad norm=2.643148751161549. num_batch_no_update =0
    Finish optimize_log_likelihood, the log likelihood is -9.406897454891597
    Params  [tensor([-0.2309], dtype=torch.float64, requires_grad=True), tensor([0.0499], dtype=torch.float64, requires_grad=True)]
    
    likelihood div batch-size? otherwise lr is batch-size*lr
    currently, we do not div batch-size, and use batch-size=64, lr=0.005

    maybe because batch_size should not too small.

    successfully recovered data-2, using 8 workers, each with batch-size-mp=64.
    for 20 ruels, 75 batches, 8-worker-mp use 7min, single-process-cache costs above 10min.

65. set time-window[Implemented], set smaller weights\intensity to get less events, to avoid too much computation.
    TODO: need to re-generate datasets. 

66. add AFTER experiment?
    [updated] remove AFTER experiment

67. larger best_N?  e.g. 8,  or even add all positive rules? (need theoreticall analysis.)

68. [implemented]add seq-avg-len 

69. modify feature calculation to O(n)

70. draw likelihood function image, verify deletting/adding rule criteria.

71. [Implemented]in featrue fucntion, filter predicate if it has time relation with target.
    add filter, reduce generating data-2 from 5h to 50min
    time_window=10
    [Generate data] Elapsed: 55.7610 min.
    [Generate data] Elapsed: 4.3923 hour.

72. generate data-2 costs long time.
    reason?
    time_window=7,8 does not fit.
    time_window=10 works 

73. in enumeration candidate rules, do not enumerate target sign, since this is redundant, by symmetricity.
    the symmetricity is "grad(R->Y) = - grad(R->notY)", it is guaranteed by (wf) term and chain-rule. 
    after calculation of log-grad, for all negative log-grad rules, revert its target sign and log-grad sign.
    Note: this is risky if there are contradictory rules in rule set. But in old verison, such risk was also not considered.
    [update] due to repeated rule bug, discard this fix.

74. fit failed for 1280, 640 samples of data-4
    2560 samples killed by system?? in [multiprocess log-grad] 40 tasks, only get 17 result and quit.
    still use cache??

75. after run setsid, check model-info and running states of program!!

76. use cp instead of torch to solve master.
    note: no l1 panelty. [Fixed: added]
    fit GT rules, 2560 samples cost 10+G memory.
    mosek solver is 3x faster than scs.
    
77. when adding contradictory rules, e.g.
    Best rule is: D ^ Not A --> E , D EQUAL Not A ^ Not A EQUAL E
    Best log-likelihood-grad(all-data) = 0.04151670803228344
    new rule added.
    Best rule is: Not D ^ Not A --> E , Not D EQUAL Not A ^ Not A EQUAL E
    Best log-likelihood-grad(all-data) = 0.039954239767975484
    new rule added.
    mosek returns:
    Problem status: The problem is primal infeasible.

    [Implemented]So we constraint candidate rules not contradictory.

78. run multiprocessing program on ubuntu system, ulimit -n = 1024,  gives:
    RuntimeError: received 0 items of ancdata.
    which means number of file descriptors is larger than limit.
    Also, this error is not outputed to stderr or stdout.
    [Fixed]solution: replace multiprocessing with torch.multiprocessing,
    and adding the following line in main function:
    ``torch.multiprocessing.set_sharing_strategy('file_system')``
    (solution comes from https://forums.fast.ai/t/runtimeerror-received-0-items-of-ancdata/48935/2)

79. most of failure results from long rules.
    [Implemented]use DFS to probe long rule?
    the old search_algorithm keeps extending a short rule, even after it is deleted.
    [Fixed]fix this bug and modfiy search_algorithm to BFS.

80. data-6 2400 samples, run out of shared-mem?
    re-run same codes, not reproduce.

81. [Implemented] strict_weight_threshold should change with dataset weight scale.

82. data variance too high?
    e.g.:
    log-likelihood-grad(all-data) mean= 0.04091, std=0.51301, Rule = Not A ^ B ^ C --> E , Not A BEFORE E ^ B BEFORE E ^ C BEFORE E
    log-likelihood-grad(all-data) mean= 0.03630, std=0.65959, Rule = D ^ B ^ C --> E , D BEFORE E ^ B BEFORE E ^ C BEFORE E
    [GT]log-likelihood-grad(all-data) mean= 0.03320, std=0.57334, Rule = A ^ B ^ C --> E , A BEFORE E ^ B BEFORE E ^ C BEFORE E
    solution: increase Best_N, increase_num_sample

83. different base for 0->1, 1->0.  model collapsed.

84. [Implemented] try various body intensity in data-[5,6,7,8,9]

85. Association rule may not apply. 
    use log-grad to filter some unrelated preds?

86. in data4 old dataset,
    GT = B-->E
    But model learns NotB --> E
    re-generate then fixed.

87. synthetic experiment on large dataset, i.e. 10-20 preds, as in mimic?

88. add static predicate, which follows:
    1)static pred only interacts with head pred.
    2)static pred is activated or inactivated by CURRENT state. (e.g. IS_EVENING(t=9pm) is indepent with history of t.)
    3)static pred has NO time-window and decay. 
    Modified functions: init(), get_feature(), generate_rule_via_column_generation(), add_one_predicate_to_existing_rule()

89. add self-activation:
    allow head appears in body.
    Modified functions: get_feature(), generate_rule_via_column_generation(), add_one_predicate_to_existing_rule()
    Bug fixed: zero-feature of self-activation rule (because rewrite head time array in time_combination_dict). 
                Wrongly assign (Head Equal Head) with large weight. (because feature calculation of EQUAL is implemented by abs(time_diff), event is wrongly activated by its current event.)


90. Fixed bug: instant head pred. Such head is activated only for a very short interval, thus theoretically, only state-1, no state-0.
    Though fake state-0 is added, we only consider state-1 in log-intensity.
    Modified functions:  intensity_log_sum()

91. Split data in week freq.
    bug: all log-grads are negeative.
    fix: instant pred state should always be 0, modify get_formula_effect() to encode this fact. 

92. weights are very small, about 0.01-0.1 
    key reason: data density. Compare crime_downtown_day, crime_all_day. the weights are 10x.


93. in mimic-data proccessing, start from 5 hours before the first treatment.

94.  Small batch_size_grad 

95. different T_max for different sample.


96. Fatal error: 
    In get_formula_effect(), counter_state == template['head_predicate_sign'] is always False, since template['head_predicate_sign'] is a LIST, not an int.
    All experiments should be re-run.

97. Important Issue:
    why log-grad all neg? Because initial base is too large.
    Solution: run 1 master before searching.

98. crime_twodays experiment result passed.
    testing crime_week. modify refreq to keep output dataset same size. 
    for crime, the num of candidate rules are small, thus use all samples for log-grad.(An inaccurate new rule will waste 2 masters, 1 master 10 mins, while log-grad is 2 min.)

99.
    for week dataset, these params work well:
    model.time_window = 20
    model.Time_tolerance = 1
    model.decay_rate = 0.1
    model.integral_resolution = 0.1

    And ablation experiment:
    (Failed) time_window = 168  --> 2021-05-16 11:42:11.361531
    dacay_rate = 0.01 --> 2021-05-16 11:42:26.773342
    integral_resolution=0.5 --> 2021-05-16 11:42:37.277589
    Time_tolerance=12 -->  2021-05-16 11:42:46.167129

    shows that the time_window is the critical factor of disappearing of static preds. 

    maybe because, static pred has always coeff 1, while other preds can have large coeff when time_window is large and decay is small.
    Solution: modify coeff to time_window/24

100. mimic data filtering, be careful of negative time(caused by substract statr_time)
     also, print out data samples to check
     drug\input should be instant preds.

101. add generate function and MAE.
but result is ...
---SampleID:9---
True: {'time': [11.416666666666666, 11.9, 16.033333333333335, 17.716666666666665, 32.016666666666666, 34.25, 35.666666666666664, 37.05, 38.03333333333333, 38.666666666666664, 38.71666666666667, 40.08333333333333, 43.65, 44.08333333333333, 56.333333333333336, 56.95, 59.45, 60.35, 60.416666666666664, 62.1, 65.11666666666667, 65.61666666666667, 66.36666666666667, 67.0, 67.0, 67.66666666666667, 80.73333333333333, 81.16666666666667, 82.53333333333333, 84.5, 86.26666666666667, 86.4, 90.05, 90.93333333333334, 91.91666666666667, 95.56666666666666, 108.23333333333333, 110.85, 111.11666666666667, 113.21666666666667, 134.58333333333334, 135.6, 135.7, 136.36666666666667, 137.03333333333333, 137.41666666666666, 137.73333333333335, 140.08333333333334, 154.1, 154.76666666666668, 159.61666666666667, 159.98333333333332, 165.25], 'state': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
Generated: {'time': array([ 11.41666667,  15.74256786,  19.52859191,  22.93964099,
        27.79744496,  31.99646861,  35.9233582 ,  39.5886063 ,
        43.37433273,  47.45848796,  51.89302167,  56.60527475,
        59.77484339,  64.18519066,  67.89670153,  72.23551738,
        75.73236063,  79.03731879,  82.7463911 ,  86.73432241,
        91.15173365,  94.8395847 ,  98.24650974, 101.4542888 ,
       105.45184987, 109.34661274, 113.20294684, 116.73053608,
       120.05957854, 123.54589987, 127.06982068, 130.12761914,
       133.44387781, 136.415315  , 139.82185744, 141.87627142,
       143.71097727, 146.01257607, 147.880168  , 149.72957801,
       151.48472145, 153.14352834, 154.54938545, 155.77459345,
       156.71179396, 157.88362522, 158.59307676, 159.4010188 ,
       159.93128251, 160.40223463, 160.69684666, 161.01344254,
       161.25248876]), 'state': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
MAE: 21.493143598091343

102. how to express self-correcting of instant pred?
     in our setting, this is impossible!
     try to add negative weights!
     [Added] at  prune_rules_with_small_weights() and select_and_add_new_rule()

103. max-intensity? 
    in generating, max-intensity sampled is inaccurate?
    maybe sample intensity at events? Or set a larger max-intensity?

104. Nan error ? 2021-05-17 20:35:44.070133 
    crime-DFS failed

105. crime-BFS failed
    intensity is too low, most of rules are negative.
    conflict rules: A->D,w=1; A^SUMMER ->D, w=-1
    i.e.static preds is source of conflict.

106. update generate function, each time, generate on head of gt samples by one event.
    mae is 3.819581479668483. (crime_all_week, only 2 rules: D->D, A^D->D)
    while HExp MAE is 0.8612458448834678

107. torch multiprocessing: all params are defaultly not shared, including variables, grad, optimizer.
    [Solution]: Tensor.share_memory_() to assign tensor as shared.(Note: the tensor.data is shared, but tensor.grad is not.)
    Also, for pool method, e.g. 3 workers do 12 tasks, worker 0 does task 0,3,6,9. But when it does task 3, it forgets state of task 0. In our exp, this state refers to optimizer state 
    [Solution]: merge 12 tasks into 3 large-tasks, then each work does 1 large-tasks.
    Bug: parallely update weights leads to model collapse! In the worset case, the actual batch-size = batch-size * worker-num.
    [Solution]: set a SMALL batch-size
    Only effective for learning base, but still NAN for the first rule.

108. inspect one-sample. hand-calculate updates to check bug.

109. exp mapping may accumulate error, change to max mapping?

110. good initialization?
    hand calculate the new optimial single weight, then optimize globally.

111. re-sacle data into 10, to constraint gradients.
    gradient is linear to scale, if sacle is too large, then prone to explode

112. add rule template to mimic. sym + drug --> sym

113. [bug Fixed] f = exp(-decay * diff) * decay. The outside decay is critical to MAE !!!
    with this formula, in crime, single variable, our model mae=0.2, comparable with HawkesExp
    

114. decay is important!! without it, abs value of features may be too large, then it is unstable for linear kernel, since intensity prones to reach 0.

115. lr of base may be smaller than lr of weights. 
    if lr of base is too large, then it is unstable, may goes to negative. 

116. mimic- #pred large, freq low.  --> use large resolution, random sampling candidate rule, to accelerate.
    crime, #pred small, freq high.  --> too slow since get_feature() is slow, change to crime_day, to lower freq.

117. DFS stopping rule bug:
    if (not is_continue) or len(added_rule_str_list) == 0: 
    should be:
    if (not is_continue):
    Since, the rule adding and deletting may be inaccurate.

118. survival data bug, for survived sample, do not add a dummy survival event, since this is recognized as death by our model.

119. add clip to urine data. patient seq is too long, but only a small segment is valid.

120. use very small lr for base, large lr for weight, small iter-num.
    random sampling log-grad with best_N=2, faster speed and better robustness against high variance.