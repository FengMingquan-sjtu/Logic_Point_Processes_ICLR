1. add testing codes in ./unit_test.
use package pytest 6.1.1, docs: https://docs.pytest.org/en/latest/contents.html
run the following cmd when you are in root directory:
pytest

2. put testing codes in several dirs to avoid repeated testing.

3. if testing codes use argparse, then it raises error when running "pytest file_path".
   since argparse will recognize file_path as its args.

4. in ./model/point_process.py line 66:
    mask = ...(transition_time < t)...
    maybe we can change to  
    mask = ...(transition_time <= t)...
    The boundary condition is meanless, and it may cause bugs.

5. notice that self-correcting logic is "A -> Not B", instead of "Not A -> B"

6. In synthetic.py we can divide preds into 3 types:
   1)target pred  2) mapped pred  3)independent pred
   type 2) is copied from 1), and type 3) is random, 
   therefore we only need to synthetic (using thining) 1) type.

   Notice that the generation order is  3) --> 1) --> 2). 
   At time t, we first randomly generate an event t_3 of 3),
   then, synthetic events of 1) in interval [t, t_3].

   Also, 1) can be further divided into 1.1)instant and 1.2)continuous. 

7. how to decide the max intensity point in a short interval? only need to calculate sign of f*w.

8. using seed, we can test correctness of synthetic data.

9.  in./model/point_process.py, intensity_log_sum() function: 
   if (not is_duration_pred) and dataset[sample_ID][target_predicate]['state'][idx]==0:
   missing "not" in first condition, missing "[idx]" in second condition.

10. In closed form integral, the most dfficult part is how to get latest pred.
    Thus currently we only implement for rule body length=1,2, but not for longer rules.

11. Integration can also use cache.

12. experiments arguments:
      hawkes: w=0.1, b=0.2;  Result: w=0.1058, b=0.1926
      sc: w=0.1, b=0.2; Result: w=0.0994, b=0.2009
      a_then_b indep instant pred: w=0.2, b=0.4, indep_intensity= 0.5. Result: w=0.1892, b=0.3749
      (modify is_duration_pred to [0,1])
      (Under current formulation, we can only set target pred to duration, but not other preds.)
      [Outdated]a_then_b indep duration pred: w=0.2, b=0.4, indep_intensity= 0.5.  Result: w=0.1314, b=0.3399

      a_then_c_or_b_then_c, instant pred, target_pred=2, w=[0.1,0.1], b=0.2;  Result: w=[0.0983, 0.1091], b=0.2264
      a_and_b_then_c, instant pred, target_pred =2, w=[0.1], b=0.2; Result: w[1.2760], b=[0.0088]

      the bug "time_tolerence must be 1e-6" disappears, maybe because here we use time_tolerence * 1.1, instead of time_tolerence * 1.001.(floating point presicion problem)

      Maybe, closed Integration on "length-2 rules" and "duration pred" are problematic.

      fix bug on length2-rule-integral,
      [Fixed]a_and_b_then_c w=[0.1], b=0.2, result: w=[0.0938], b=[0.2102]
      fix bug on Equal: in synthetic Equal, new events should not add time_tolerence * 1.1, since that prevent model from capturing this event.
      a_then_b_equal, w=[0.1], b=0.2, time_tolerence=5, result: w=[0.0350], b=[0.0771]
      [Fixed]a_then_b_equal, w=[0.1], b=0.2, time_tolerence=5, result: w=[0.0824], b=[0.2193]

      [Fixed]a_then_b duration pred, w=[0.1], b=[0.4], Result: w=[0.0800], b=[0.3573]
      Notice that in duration case, fe can be negative, so w should be much smaller than b. 
      Otherwise lambda can be negative, leads to wrong closed integral.

13.use CVXPY package to solve master problem:
   due to DCP requirements (see https://www.cvxpy.org/tutorial/dcp/index.html#dcp)
   we skip non_negative_mapping part, in ./model/master_problem.py line 127
   Experiment results:
   Hawkes, GT={w:0.1, b:0.2}, OPT={w:0.103, b:0.191}
   SC, GT={w:0.1, b:0.2}, OPT={w:0.100, b:0.201}
   a_then_b, GT={w:0.1, b:0.4}, OPT={w:0.066, b:0.343}
   
   Speed of CVXPY is faster than PyTorch.

14. 目前对subproblem的假设：targetpred总是出现在规则的最后、所有时序关系总是以rule中第一个pred为基准。
   采用枚举的方法来选择最优的rule
   tips：1）改动logic（增加、删除规则）之后要记得update PP.template

15. 发现sub-problem的obj是nan，导致迭代第一轮就结束。
   原因是sub-problem obj term1 的分母的第0个数据恒为0

16. 
hawkes数据集 w=0.1, b=0.2。
初始rule为(array([0, 0]), array([  0, 100]))
第一轮迭代， master result：
w = [-7.33484268e-10]
b = [0.    0.371]
lambda = 1.6882037755368875e-08
sub-porblem, 
obj = -4000951681050.4585
(array([0, 1]), array([   0, 1000]), array([1., 1.]))
obj = -4000951663581.11
(array([1, 0]), array([  0, 100]), array([1., 1.]))
obj = -4000951645611.754
(array([1, 0]), array([   0, 1000]), array([1., 1.]))
obj = -4000951663581.102
(array([1, 1]), array([  0, 100]), array([1., 1.]))
obj = -4000951681550.458
(array([1, 1]), array([   0, 1000]), array([1., 1.]))
obj = -4000951663581.11

它返回了(array([1, 1]), array([  0, 100]), array([1., 1.]))
这个返回值是不正确的。
这里所有rule的obj绝对值都很大，并且除了最低5位数以外都一样，为什么？

第二轮迭代：
规则集为：
(array([0, 0]), array([  0, 100]))
(array([1, 1]), array([  0, 100]))

Master result：
w = [-9.90928814e-10  1.06282436e-01]
b = [0.         0.18001734]
lambda = 5.2167890995233067e-08

为什么这里恰好能解出w=0.1, b=0.2 ??


Sub problem 过程：
new_rule_triplet = (array([0, 1]), array([  0, 100]), array([1., 1.]))
obj = tensor(16396.7493, dtype=torch.float64)
new_rule_triplet = (array([0, 1]), array([   0, 1000]), array([1., 1.]))
obj = tensor(33866.0979, dtype=torch.float64)
new_rule_triplet = (array([1, 0]), array([  0, 100]), array([1., 1.]))
obj = tensor(51835.4539, dtype=torch.float64)
new_rule_triplet = (array([1, 0]), array([   0, 1000]), array([1., 1.]))
obj = tensor(33866.1055, dtype=torch.float64)

全都大于0，所以程序结束了。
正确的rule是(array([0, 1]), array([  0, 100])），但是没被选中。

   

neighborhood 相同的rule 提取出的feature绝对值必然相同， 符号由target符号决定。
规则[0,1] 与 规则[1,1] 所提取特征几乎相同，这是因为hawkes的事件都是瞬时的，A的0和1几乎重合。
所以模型在hawkes输出[1,1]是可以理解的
但是在self-correcting的结果完全错误？


17.
sub-problem公式第一项分母加上bias，第二项符号为正。

18. feature-cache feature_integral_cache 导致sub-problem出错

19. pp.filtering没有把t0=t筛掉，所以EQUAL总是被优先选择。改掉就好了

20. sp的obj有时会仅仅比0小一点点，这些情况应该忽略。

21. sp的计算冗余：intensity、feature、integral等均可多次复用。

22. 解a_then_b时，发现结果如下：
w =  [6.16242876e-02  1.38375718e-01]
b =  [0.         0.32713167]
Logic rules: 
(array([0, 1]), array([  0, 100]))
(array([0, 1]), array([   0, 1000]))
为什么会出现EQUAL的这条rule，并且权重还很大呢？
这是PP-likelihood的漏洞。
EQUAL 对于likelihood的sum log lambda这一项毫无贡献，但是对第二项integral有正的贡献，所以模型会尽可能调高它的权重。
这样的漏洞应该对所有 持续性predicate生效。

12月18日，已通过了Hawkes、Self-correcting的测试。没有通过a_then_b测试。